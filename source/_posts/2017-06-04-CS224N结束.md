---
layout: post
title: CS224N总结
date: 2017-06-04
---

# CS224n总结，收获，遗憾

一转眼都六月份，家里大同的太阳已经很热了。CS224n拖了很长时间，本来打算把作业是做完，然后再写总结的。但无奈身体一直不好，你吃没有精力把作业是做完。时间不等人，再不总结就都忘光了。说是总结，其实就是把这么长时间以来，留在脑子里的剩下这一点知识再稍微串一下吧遗留下来的才是最重要的精华。

对照课程表，深入学习自然处理最开始部分就是谷歌的那个论文。word2vec

通过把one_hot编码的单词转化成底维空间中的向量，极大的节约了空间，起来，提升了后续算法的准确率。不过后来glove算法的效果似乎更胜一筹。

字嵌入词嵌入应该是所有的自然语言处理中，所有的第一步吧。模型的第一层，作业一是w2v的推导

之后就课程主要是讲述了深度学习模型钱向传播。重点推倒了各种的反向传播算法。作业二就是它的应用，依存句法分析。

他们期中考试完之后的那节课讲的是tf，有时可见实践与前沿

再之后便进入了经典的rnn循环神经网络模型。

从最基本的单个神经元。到逃stm，GRU这些模型。这些模型在作业三中都经过了推倒与tf实现。讲课中教师主要重点在于直观的理解

之后便是注意力，极致和各种神经网络的变形。在做作业似的时候才发现在这里，我理解不是很扎实。

这里便到了seq2seq模型，翻译nmt，语音识别（这节课的老师请了一个大牛来讲，但是我完全没有听懂。）

最后便是深度神经网络与自然处理的其他一些前沿的研究，还有他遇到的一些问题。我在课程的学习过程中深刻的体会到了斯坦福大学对学生教育的重视和认真。也感到了好大学，对学生资源的投入。而且有那么多微软赞助的GPU资源。不过斯坦福大学的学生也令人佩服。真不知道他们是如何在三个月的时间中掌握到那么扎实的程度。

他们其中是有总结课的，可惜期末并没有。对于我这种之前基本上没有深度学习经验的人来说，怎么可还是有一些困难。但是总体算是啊，我入了门。但是之后还是应该继续学习，或者重新看一遍本故而知新。